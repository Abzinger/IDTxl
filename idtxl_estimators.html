<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    
    <title>Information theoretic estimators &mdash; IDTxl 0.1 documentation</title>
    <meta name="description" content="">
    <meta name="author" content="">

    

<link rel="stylesheet" href="_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" id="current-theme" href="_static/css/bootstrap3/bootswatch-spacelab.css" type="text/css" />
<link rel="stylesheet" id="current-adjust-theme" href="_static/css/adjust_theme/bootswatch-spacelab.css" type="text/css" />

<link rel="stylesheet" href="_static/css/font-awesome.min.css">

<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
</style>

<link rel="stylesheet" href="_static/css/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    './',
            VERSION:     '0.1',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
    <script type="text/javascript" src="_static/js/jquery.min.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/bootstrap3.min.js"></script>
<script type="text/javascript" src="_static/js/jquery.cookie.min.js"></script>
<script type="text/javascript" src="_static/js/basicstrap.js"></script>
<script type="text/javascript">
</script>
    <link rel="top" title="IDTxl 0.1 documentation" href="index.html" />
    <link rel="next" title="Helper functions" href="idtxl_helper.html" />
    <link rel="prev" title="Algorithms for network comparison" href="idtxl_network_comparison.html" /> 
  </head>
  <body role="document">
    <div id="navbar-top" class="navbar navbar-fixed-top navbar-default" role="navigation" aria-label="top navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">IDTxl 0.1 documentation</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
              <li class="dropdown visible-xs">
                <a role="button" id="localToc" data-toggle="dropdown" data-target="#" href="#">Table Of Contents <b class="caret"></b></a>
                <ul class="dropdown-menu localtoc sp-localtoc" role="menu" aria-labelledby="localToc">
                <ul>
<li><a class="reference internal" href="#">Information theoretic estimators</a><ul>
<li><a class="reference internal" href="#module-idtxl.estimators_jidt">idtxl.estimators_jidt module</a></li>
<li><a class="reference internal" href="#module-idtxl.estimators_opencl">idtxl.estimators_opencl module</a></li>
<li><a class="reference internal" href="#module-idtxl.estimators_pid">idtxl.estimators_pid module</a></li>
</ul>
</li>
</ul>

                </ul>
              </li>

            
              <li><a href="idtxl_network_comparison.html" title="Algorithms for network comparison" accesskey="P">previous </a></li>
              <li><a href="idtxl_helper.html" title="Helper functions" accesskey="N">next </a></li>
              <li><a href="py-modindex.html" title="Python Module Index" >modules </a></li>
              <li><a href="genindex.html" title="General Index" accesskey="I">index </a></li>
            
            <li class="visible-xs"><a href="_sources/idtxl_estimators.txt" rel="nofollow">Show Source</a></li>

            <li class="visible-xs">
                <form class="search form-search form-inline navbar-form navbar-right sp-searchbox" action="search.html" method="get">
                  <div class="input-append input-group">
                    <input type="text" class="search-query form-control" name="q" placeholder="Search...">
                    <span class="input-group-btn">
                    <input type="submit" class="btn" value="Go" />
                    </span>
                  </div>
                  <input type="hidden" name="check_keywords" value="yes" />
                  <input type="hidden" name="area" value="default" />
                </form>
            </li>

            

          </ul>

        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container">

      <!-- row -->
      <div class="row">
        
        

        <div class="col-md-9" id="content-wrapper">
          <div class="document" role="main">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <div class="section" id="information-theoretic-estimators">
<h1>Information theoretic estimators<a class="headerlink" href="#information-theoretic-estimators" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-idtxl.estimators_jidt">
<span id="idtxl-estimators-jidt-module"></span><h2>idtxl.estimators_jidt module<a class="headerlink" href="#module-idtxl.estimators_jidt" title="Permalink to this headline">¶</a></h2>
<p>Provide JIDT estimators.</p>
<dl class="class">
<dt id="idtxl.estimators_jidt.JidtDiscrete">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtDiscrete</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscrete"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscrete" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract class for implementation of discrete JIDT-estimators.</p>
<p>Abstract class for implementation of plug-in JIDT-estimators for discrete
data. Child classes implement estimators for mutual information (MI),
conditional mutual information (CMI), actice information storage (AIS), and
transfer entropy (TE). See parent class for references.</p>
<p>Set common estimation parameters for discrete JIDT-estimators. For usage of
these estimators see documentation for the child classes.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;discretise_method&#8217; - if and how to discretise incoming
continuous variables to discrete values, can be &#8216;max_ent&#8217; for
maximum entropy binning, &#8216;equal&#8217; for equal size bins, and &#8216;none&#8217;
if no binning is required (default=&#8217;none&#8217;)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Discrete JIDT estimators require the data&#8217;s alphabet size for
instantiation. Hence, opposed to the Kraskov and Gaussian estimators,
the JAVA class is added to the object instance, while for Kraskov/
Gaussian estimators an instance of that class is added (because for the
latter, objects can be instantiated independent of data properties).</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscrete.estimate_surrogates_analytic">
<code class="descname">estimate_surrogates_analytic</code><span class="sig-paren">(</span><em>n_perm=200</em>, <em>**data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscrete.estimate_surrogates_analytic"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscrete.estimate_surrogates_analytic" title="Permalink to this definition">¶</a></dt>
<dd><p>Return estimate of the analytical surrogate distribution.</p>
<p>This method must be implemented because this class&#8217;
is_analytic_null_estimator() method returns true.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>n_perms <span class="classifier-delimiter">:</span> <span class="classifier">int [optional]</span></dt>
<dd>number of permutations (default=200)</dd>
<dt>data <span class="classifier-delimiter">:</span> <span class="classifier">dict of numpy arrays</span></dt>
<dd>realisations of random variables required for the calculation
(varies between estimators, e.g. 2 variables for MI, 3 for
CMI). Formatted as per the estimate method for this estimator.</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>n_perm surrogates of the average MI/CMI/TE over all samples
under the null hypothesis of no relationship between var1 and
var2 (in the context of conditional)</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscrete.get_analytic_distribution">
<code class="descname">get_analytic_distribution</code><span class="sig-paren">(</span><em>**data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscrete.get_analytic_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscrete.get_analytic_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a JIDT AnalyticNullDistribution object.</p>
<p>Required so that our estimate_surrogates_analytic method can use the
common_estimate_surrogates_analytic() method, where data is formatted
as per the estimate method for this estimator.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>data <span class="classifier-delimiter">:</span> <span class="classifier">dict of numpy arrays</span></dt>
<dd>realisations of random variables required for the calculation
(varies between estimators, e.g. 2 variables for MI, 3 for
CMI). Formatted as per the estimate method for this estimator.</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>Java object</dt>
<dd>JIDT calculator that was used here</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtDiscreteAIS">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtDiscreteAIS</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteAIS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteAIS" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate AIS with JIDT&#8217;s discrete-variable implementation.</p>
<p>Calculate the active information storage (AIS) for one process. Call JIDT
via jpype and use the discrete estimator. See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;discretise_method&#8217; - if and how to discretise incoming
continuous variables to discrete values, can be &#8216;max_ent&#8217; for
maximum entropy binning, &#8216;equal&#8217; for equal size bins, and &#8216;none&#8217;
if no binning is required (default=&#8217;none&#8217;)</li>
<li>&#8216;num_discrete_bins&#8217; - number of discrete bins/levels or the base
of each dimension of the discrete variables (default=2). If set,
this parameter overwrites/sets &#8216;alph&#8217;</li>
<li>&#8216;history&#8217; - number of samples in the target&#8217;s past used as
embedding</li>
<li>&#8216;alph&#8217; - number of discrete bins/levels for var1 (default=2 , or
the value set for &#8216;num_discrete_bins&#8217;)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteAIS.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>process</em>, <em>return_calc=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteAIS.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteAIS.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate active information storage.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>process <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations as either a 2D numpy array where array dimensions
represent [realisations x variable dimension] or a 1D array
representing [realisations], array type can be float (requires
discretisation) or int</dd>
<dt>return_calc <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>return the calculator used here as well as the numeric
calculated value(s)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average AIS over all samples or local AIS for individual
samples if &#8216;local_values&#8217;=True</dd>
<dt>Java object</dt>
<dd>JIDT calculator that was used here. Only returned if
return_calc was set.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteAIS.get_analytic_distribution">
<code class="descname">get_analytic_distribution</code><span class="sig-paren">(</span><em>process</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteAIS.get_analytic_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteAIS.get_analytic_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a JIDT AnalyticNullDistribution object.</p>
<p>Required so that our estimate_surrogates_analytic method can use the
common_estimate_surrogates_analytic() method, where data is formatted
as per the estimate method for this estimator.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>process <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations as either a 2D numpy array where array dimensions
represent [realisations x variable dimension] or a 1D array
representing [realisations], array type can be float (requires
discretisation) or int</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>Java object</dt>
<dd>JIDT calculator that was used here</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtDiscreteCMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtDiscreteCMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteCMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteCMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate CMI with JIDT&#8217;s implementation for discrete variables.</p>
<p>Calculate the conditional mutual information between two variables given
the third. Call JIDT via jpype and use the discrete estimator. See parent
class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=&#8217;false&#8217;)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;discretise_method&#8217; - if and how to discretise incoming
continuous variables to discrete values, can be &#8216;max_ent&#8217; for
maximum entropy binning, &#8216;equal&#8217; for equal size bins, and &#8216;none&#8217;
if no binning is required (default=&#8217;none&#8217;)</li>
<li>&#8216;num_discrete_bins&#8217; - number of discrete bins/levels or the base
of each dimension of the discrete variables (default=2). If set,
this parameter overwrites/sets &#8216;alph1&#8217;, &#8216;alph2&#8217; and &#8216;alphc&#8217;</li>
<li>&#8216;alph1&#8217; - number of discrete bins/levels for var1 (default=2, or
the value set for &#8216;num_discrete_bins&#8217;)</li>
<li>&#8216;alph2&#8217; - number of discrete bins/levels for var2 (default=2, or
the value set for &#8216;num_discrete_bins&#8217;)</li>
<li>&#8216;alphc&#8217; - number of discrete bins/levels for conditional
(default=2, or the value set for &#8216;num_discrete_bins&#8217;)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteCMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>conditional=None</em>, <em>return_calc=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteCMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteCMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate conditional mutual information.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations], array type can be
float (requires discretisation) or int</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>conditional <span class="classifier-delimiter">:</span> <span class="classifier">numpy array [optional]</span></dt>
<dd>realisations of the conditioning variable (similar to var), if
no conditional is provided, return MI between var1 and var2</dd>
<dt>return_calc <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>return the calculator used here as well as the numeric
calculated value(s)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average CMI over all samples or local CMI for individual
samples if &#8216;local_values&#8217;=True</dd>
<dt>Java object</dt>
<dd>JIDT calculator that was used here. Only returned if
return_calc was set.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteCMI.get_analytic_distribution">
<code class="descname">get_analytic_distribution</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>conditional=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteCMI.get_analytic_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteCMI.get_analytic_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a JIDT AnalyticNullDistribution object.</p>
<p>Required so that our estimate_surrogates_analytic method can use the
common_estimate_surrogates_analytic() method, where data is formatted
as per the estimate method for this estimator.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations], array type can be
float (requires discretisation) or int</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>conditional <span class="classifier-delimiter">:</span> <span class="classifier">numpy array [optional]</span></dt>
<dd>realisations of the conditioning variable (similar to var), if
no conditional is provided, return MI between var1 and var2</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>Java object</dt>
<dd>JIDT calculator that was used here</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtDiscreteMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtDiscreteMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate MI with JIDT&#8217;s discrete-variable implementation.</p>
<p>Calculate the mutual information (MI) between two variables. Call JIDT via
jpype and use the discrete estimator. See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;discretise_method&#8217; - if and how to discretise incoming
continuous variables to discrete values, can be &#8216;max_ent&#8217; for
maximum entropy binning, &#8216;equal&#8217; for equal size bins, and &#8216;none&#8217;
if no binning is required (default=&#8217;none&#8217;)</li>
<li>&#8216;num_discrete_bins&#8217; - number of discrete bins/levels or the base
of each dimension of the discrete variables (default=2). If set,
this parameter overwrites/sets &#8216;alph1&#8217; and &#8216;alph2&#8217;</li>
<li>&#8216;alph1&#8217; - number of discrete bins/levels for var1 (default=2, or
the value set for &#8216;num_discrete_bins&#8217;)</li>
<li>&#8216;alph2&#8217; - number of discrete bins/levels for var2 (default=2, or
the value set for &#8216;num_discrete_bins&#8217;)</li>
<li>&#8216;lag&#8217; - time difference in samples to calculate the lagged MI
between processes (default=0)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>return_calc=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate mutual information.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations], array type can be
float (requires discretisation) or int</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>return_calc <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>return the calculator used here as well as the numeric
calculated value(s)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average MI over all samples or local MI for individual
samples if &#8216;local_values&#8217;=True</dd>
<dt>Java object</dt>
<dd>JIDT calculator that was used here. Only returned if
return_calc was set.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteMI.get_analytic_distribution">
<code class="descname">get_analytic_distribution</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteMI.get_analytic_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteMI.get_analytic_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a JIDT AnalyticNullDistribution object.</p>
<p>Required so that our estimate_surrogates_analytic method can use the
common_estimate_surrogates_analytic() method, where data is formatted
as per the estimate method for this estimator.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations], array type can be
float (requires discretisation) or int</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>Java object</dt>
<dd>JIDT calculator that was used here</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtDiscreteTE">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtDiscreteTE</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteTE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteTE" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate TE with JIDT&#8217;s implementation for discrete variables.</p>
<p>Calculate the transfer entropy between two time series processes.
Call JIDT via jpype and use the discrete estimator. Transfer entropy is
defined as the conditional mutual information between the source&#8217;s past
state and the target&#8217;s current value, conditional on the target&#8217;s past.
See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;discretise_method&#8217; - if and how to discretise incoming
continuous variables to discrete values, can be &#8216;max_ent&#8217; for
maximum entropy binning, &#8216;equal&#8217; for equal size bins, and &#8216;none&#8217;
if no binning is required (default=&#8217;none&#8217;)</li>
<li>&#8216;num_discrete_bins&#8217; - number of discrete bins/levels or the base
of each dimension of the discrete variables (default=2). If set,
this parameter overwrites/sets &#8216;alph1&#8217; and &#8216;alph2&#8217;</li>
<li>&#8216;alph1&#8217; - number of discrete bins/levels for source
(default=2, or the value set for &#8216;num_discrete_bins&#8217;)</li>
<li>&#8216;alph2&#8217; - number of discrete bins/levels for target
(default=2, or the value set for &#8216;num_discrete_bins&#8217;)</li>
<li>&#8216;history_target&#8217; - number of samples in the target&#8217;s past used as
embedding</li>
<li>&#8216;history_source&#8217; - number of samples in the source&#8217;s past used as
embedding (default=same as the target history)</li>
<li>&#8216;tau_source&#8217; - source&#8217;s embedding delay (default=1)</li>
<li>&#8216;tau_target&#8217; - target&#8217;s embedding delay (default=1)</li>
<li>&#8216;source_target_delay&#8217; - information transfer delay between source
and target (default=1)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteTE.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>source</em>, <em>target</em>, <em>return_calc=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteTE.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteTE.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate transfer entropy from a source to a target variable.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>source <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of source variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations], array type can be
float (requires discretisation) or int</dd>
<dt>target <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of target variable (similar to var1)</dd>
<dt>return_calc <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>return the calculator used here as well as the numeric
calculated value(s)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average TE over all samples or local TE for individual
samples if &#8216;local_values&#8217;=True</dd>
<dt>Java object</dt>
<dd>JIDT calculator that was used here. Only returned if
return_calc was set.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="idtxl.estimators_jidt.JidtDiscreteTE.get_analytic_distribution">
<code class="descname">get_analytic_distribution</code><span class="sig-paren">(</span><em>source</em>, <em>target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtDiscreteTE.get_analytic_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtDiscreteTE.get_analytic_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a JIDT AnalyticNullDistribution object.</p>
<p>Required so that our estimate_surrogates_analytic method can use the
common_estimate_surrogates_analytic() method, where data is formatted
as per the estimate method for this estimator.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>source <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of source variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations], array type can be
float (requires discretisation) or int</dd>
<dt>target <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of target variable (similar to var1)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>Java object</dt>
<dd>JIDT calculator that was used here</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtEstimator">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtEstimator</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtEstimator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract class for implementation of JIDT estimators.</p>
<p>Abstract class for implementation of JIDT estimators, child classes
implement estimators for mutual information (MI), conditional mutual
information (CMI), active information storage (AIS), transfer entropy (TE)
using the Kraskov-Grassberger-Stoegbauer estimator for continuous data,
plug-in estimators for discrete data, and Gaussian estimators for
continuous Gaussian data.</p>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>Lizier, Joseph T. (2014). JIDT: an information-theoretic toolkit for
studying the dynamics of complex systems. Front Robot AI, 1(11).</li>
<li>Kraskov, A., Stoegbauer, H., &amp; Grassberger, P. (2004). Estimating
mutual information. Phys Rev E, 69(6), 066138.</li>
<li>Lizier, Joseph T., Mikhail Prokopenko, and Albert Y. Zomaya. (2012).
Local measures of information storage in complex distributed
computation. Inform Sci, 208, 39-54.</li>
<li>Schreiber, T. (2000). Measuring information transfer. Phys Rev Lett,
85(2), 461.</li>
</ul>
</div></blockquote>
<p>Set common estimation parameters for JIDT estimators. For usage of these
estimators see documentation for the child classes.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtGaussian">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtGaussian</code><span class="sig-paren">(</span><em>CalcClass</em>, <em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussian" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract class for implementation of JIDT Gaussian-estimators.</p>
<p>Abstract class for implementation of JIDT Gaussian-estimators, child
classes implement estimators for mutual information (MI), conditional
mutual information (CMI), actice information storage (AIS), transfer
entropy (TE) using JIDT&#8217;s Gaussian estimator for continuous data. See
parent class for references.</p>
<p>Set common estimation parameters for JIDT Kraskov-estimators. For usage of
these estimators see documentation for the child classes.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>CalcClass <span class="classifier-delimiter">:</span> <span class="classifier">JAVA class</span></dt>
<dd>JAVA class returned by jpype.JPackage</dd>
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtGaussian.estimate_surrogates_analytic">
<code class="descname">estimate_surrogates_analytic</code><span class="sig-paren">(</span><em>n_perm=200</em>, <em>**data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussian.estimate_surrogates_analytic"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussian.estimate_surrogates_analytic" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate the surrogate distribution analytically.
This method must be implemented because this class&#8217;
is_analytic_null_estimator() method returns true</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><p class="first">n_perms : number of permutations (default 200)
data : array of numpy arrays</p>
<blockquote class="last">
<div>realisations of random variables required for the calculation
(varies between estimators, e.g. 2 variables for MI, 3 for
CMI). Formatted as per estimate_mult for this estimator.</div></blockquote>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>n_perm surrogates of the average MI/CMI/TE over all samples
under the null hypothesis of no relationship between var1 and
var2 (in the context of conditional)</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="idtxl.estimators_jidt.JidtGaussian.get_analytic_distribution">
<code class="descname">get_analytic_distribution</code><span class="sig-paren">(</span><em>**data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussian.get_analytic_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussian.get_analytic_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a JIDT AnalyticNullDistribution object.</p>
<p>Required so that our estimate_surrogates_analytic method can use the
common_estimate_surrogates_analytic() method, where data is formatted
as per the estimate method for this estimator.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>data <span class="classifier-delimiter">:</span> <span class="classifier">dict of numpy arrays</span></dt>
<dd>realisations of random variables required for the calculation
(varies between estimators, e.g. 2 variables for MI, 3 for
CMI). Formatted as per the estimate method for this estimator.</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>Java object</dt>
<dd>JIDT calculator that was used here</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtGaussianAIS">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtGaussianAIS</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianAIS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianAIS" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate active information storage with JIDT&#8217;s Gaussian implementation.</p>
<p>Calculate active information storage (AIS) for some process using JIDT&#8217;s
implementation of the Gaussian estimator. AIS is defined as the
mutual information between the processes&#8217; past state and current value.</p>
<p>The past state needs to be defined in the settings dictionary, where a past
state is defined as a uniform embedding with parameters history and tau.
The history describes the number of samples taken from a processes&#8217; past,
tau describes the embedding delay, i.e., the spacing between every two
samples from the processes&#8217; past.</p>
<p>See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
<li>&#8216;history&#8217; - number of samples in the processes&#8217; past used as
embedding</li>
<li>&#8216;tau&#8217; - the processes&#8217; embedding delay (default=1)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the AIS estimator to save
computation time. The Theiler window ignores trial boundaries. The
AIS estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtGaussianAIS.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>process</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianAIS.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianAIS.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate active information storage.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>process <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average AIS over all samples or local AIS for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtGaussianCMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtGaussianCMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianCMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianCMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate conditional mutual infor with JIDT&#8217;s Gaussian implementation.</p>
<p>Computes the differential conditional mutual information of two
multivariate sets of observations, conditioned on another, assuming that
the probability distribution function for these observations is a
multivariate Gaussian distribution.
Call JIDT via jpype and use
ConditionalMutualInfoCalculatorMultiVariateGaussian estimator.
If no conditional is given (is None), the function returns the mutual
information between var1 and var2.</p>
<p>See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the CMI estimator to save
computation time. The Theiler window ignores trial boundaries. The
CMI estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtGaussianCMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>conditional=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianCMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianCMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate conditional mutual information.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>conditional <span class="classifier-delimiter">:</span> <span class="classifier">numpy array [optional]</span></dt>
<dd>realisations of the conditioning variable (similar to var), if
no conditional is provided, return MI between var1 and var2</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average CMI over all samples or local CMI for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="idtxl.estimators_jidt.JidtGaussianCMI.get_analytic_distribution">
<code class="descname">get_analytic_distribution</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>conditional=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianCMI.get_analytic_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianCMI.get_analytic_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a JIDT AnalyticNullDistribution object.</p>
<p>Required so that our estimate_surrogates_analytic method can use the
common_estimate_surrogates_analytic() method, where data is formatted
as per the estimate method for this estimator.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>conditional <span class="classifier-delimiter">:</span> <span class="classifier">numpy array [optional]</span></dt>
<dd>realisations of the conditioning variable (similar to var), if
no conditional is provided, return MI between var1 and var2</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>Java object</dt>
<dd>JIDT calculator that was used here</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtGaussianMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtGaussianMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate mutual information with JIDT&#8217;s Gaussian implementation.</p>
<p>Calculate the mutual information between two variables. Call JIDT via jpype
and use the Gaussian estimator. See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
<li>&#8216;lag&#8217; - time difference in samples to calculate the lagged MI
between processes (default=0)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the MI estimator to save
computation time. The Theiler window ignores trial boundaries. The
MI estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtGaussianMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate mutual information.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average MI over all samples or local MI for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtGaussianTE">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtGaussianTE</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianTE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianTE" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate transfer entropy with JIDT&#8217;s Gaussian implementation.</p>
<p>Calculate transfer entropy between a source and a target variable using
JIDT&#8217;s implementation of the Gaussian estimator. Transfer entropy is
defined as the conditional mutual information between the source&#8217;s past
state and the target&#8217;s current value, conditional on the target&#8217;s past.</p>
<p>Past states need to be defined in the settings dictionary, where a past
state is defined as a uniform embedding with parameters history and tau.
The history describes the number of samples taken from a variable&#8217;s past,
tau descrices the embedding delay, i.e., the spacing between every two
samples from the processes&#8217; past.</p>
<p>See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
<li>&#8216;history_target&#8217; - number of samples in the target&#8217;s past used as
embedding</li>
<li>&#8216;history_source&#8217; - number of samples in the source&#8217;s past used as
embedding (default=same as the target history)</li>
<li>&#8216;tau_source&#8217; - source&#8217;s embedding delay (default=1)</li>
<li>&#8216;tau_target&#8217; - target&#8217;s embedding delay (default=1)</li>
<li>&#8216;source_target_delay&#8217; - information transfer delay between source
and target (default=1)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the CMI estimator to save
computation time. The Theiler window ignores trial boundaries. The
CMI estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtGaussianTE.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>source</em>, <em>target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtGaussianTE.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtGaussianTE.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate transfer entropy from a source to a target variable.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>source <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of source variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of target variable (similar to var1)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average TE over all samples or local TE for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtKraskov">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtKraskov</code><span class="sig-paren">(</span><em>CalcClass</em>, <em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskov"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskov" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract class for implementation of JIDT Kraskov-estimators.</p>
<p>Abstract class for implementation of JIDT Kraskov-estimators, child classes
implement estimators for mutual information (MI), conditional mutual
information (CMI), actice information storage (AIS), transfer entropy (TE)
using the Kraskov-Grassberger-Stoegbauer estimator for continuous data.
See parent class for references.</p>
<p>Set common estimation parameters for JIDT Kraskov-estimators. For usage of
these estimators see documentation for the child classes.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>CalcClass <span class="classifier-delimiter">:</span> <span class="classifier">JAVA class</span></dt>
<dd>JAVA class returned by jpype.JPackage</dd>
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtKraskovAIS">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtKraskovAIS</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovAIS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovAIS" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate active information storage with JIDT&#8217;s Kraskov implementation.</p>
<p>Calculate active information storage (AIS) for some process using JIDT&#8217;s
implementation of the Kraskov type 1 estimator. AIS is defined as the
mutual information between the processes&#8217; past state and current value.</p>
<p>The past state needs to be defined in the settings dictionary, where a past
state is defined as a uniform embedding with parameters history and tau.
The history describes the number of samples taken from a processes&#8217; past,
tau describes the embedding delay, i.e., the spacing between every two
samples from the processes&#8217; past.</p>
<p>See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
<li>&#8216;history&#8217; - number of samples in the processes&#8217; past used as
embedding</li>
<li>&#8216;tau&#8217; - the processes&#8217; embedding delay (default=1)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the AIS estimator to save
computation time. The Theiler window ignores trial boundaries. The
AIS estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtKraskovAIS.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>process</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovAIS.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovAIS.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate active information storage.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>process <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average AIS over all samples or local AIS for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtKraskovCMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtKraskovCMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovCMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovCMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate conditional mutual inform with JIDT&#8217;s Kraskov implementation.</p>
<p>Calculate the conditional mutual information (CMI) between three variables.
Call JIDT via jpype and use the Kraskov 1 estimator. If no conditional is
given (is None), the function returns the mutual information between var1
and var2. See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the CMI estimator to save
computation time. The Theiler window ignores trial boundaries. The
CMI estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtKraskovCMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>conditional=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovCMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovCMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate conditional mutual information.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>conditional <span class="classifier-delimiter">:</span> <span class="classifier">numpy array [optional]</span></dt>
<dd>realisations of the conditioning variable (similar to var), if
no conditional is provided, return MI between var1 and var2</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average CMI over all samples or local CMI for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtKraskovMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtKraskovMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate mutual information with JIDT&#8217;s Kraskov implementation.</p>
<p>Calculate the mutual information between two variables. Call JIDT via jpype
and use the Kraskov 1 estimator. See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
<li>&#8216;lag&#8217; - time difference in samples to calculate the lagged MI
between processes (default=0)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the MI estimator to save
computation time. The Theiler window ignores trial boundaries. The
MI estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtKraskovMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate mutual information.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average MI over all samples or local MI for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_jidt.JidtKraskovTE">
<em class="property">class </em><code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">JidtKraskovTE</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovTE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovTE" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate transfer entropy with JIDT&#8217;s Kraskov implementation.</p>
<p>Calculate transfer entropy between a source and a target variable using
JIDT&#8217;s implementation of the Kraskov type 1 estimator. Transfer entropy is
defined as the conditional mutual information between the source&#8217;s past
state and the target&#8217;s current value, conditional on the target&#8217;s past.</p>
<p>Past states need to be defined in the settings dictionary, where a past
state is defined as a uniform embedding with parameters history and tau.
The history describes the number of samples taken from a variable&#8217;s past,
tau descrices the embedding delay, i.e., the spacing between every two
samples from the processes&#8217; past.</p>
<p>See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">sets estimation parameters:</p>
<ul class="last simple">
<li>&#8216;debug&#8217; - return debug information when calling JIDT.
(Boolean, default=False)</li>
<li>&#8216;local_values&#8217; - return local TE instead of average TE
(default=False)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;num_threads&#8217; - number of threads used for estimation
(default=&#8217;USE_ALL&#8217;, not that this uses <em>all</em> available threads
on the current machine)</li>
<li>&#8216;history_target&#8217; - number of samples in the target&#8217;s past used as
embedding</li>
<li>&#8216;history_source&#8217; - number of samples in the source&#8217;s past used as
embedding (default=same as the target history)</li>
<li>&#8216;tau_source&#8217; - source&#8217;s embedding delay (default=1)</li>
<li>&#8216;tau_target&#8217; - target&#8217;s embedding delay (default=1)</li>
<li>&#8216;source_target_delay&#8217; - information transfer delay between source
and target (default=1)</li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>Some technical details: JIDT normalises over realisations, IDTxl
normalises over raw data once, outside the CMI estimator to save
computation time. The Theiler window ignores trial boundaries. The
CMI estimator does add noise to the data as a default. To make analysis
runs replicable set noise_level to 0.</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_jidt.JidtKraskovTE.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>source</em>, <em>target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#JidtKraskovTE.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.JidtKraskovTE.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate transfer entropy from a source to a target variable.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>source <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of source variable, either a 2D numpy array where
array dimensions represent [realisations x variable dimension]
or a 1D array representing [realisations]</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of target variable (similar to var1)</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average TE over all samples or local TE for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="idtxl.estimators_jidt.common_estimate_surrogates_analytic">
<code class="descclassname">idtxl.estimators_jidt.</code><code class="descname">common_estimate_surrogates_analytic</code><span class="sig-paren">(</span><em>estimator</em>, <em>n_perm=200</em>, <em>**data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_jidt.html#common_estimate_surrogates_analytic"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_jidt.common_estimate_surrogates_analytic" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate the surrogate distribution analytically for JidtEstimator.</p>
<p>Estimate the surrogate distribution analytically for a JidtEstimator
which is_analytic_null_estimator(), by sampling estimates at random
p-values in the analytic distribution.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">a JidtEstimator object, which returns True to a call to</span></dt>
<dd>its is_analytic_null_estimator() method</dd>
</dl>
<p>n_perms : number of permutations (default 200)
data : array of numpy arrays</p>
<blockquote class="last">
<div>realisations of random variables required for the calculation
(varies between estimators, e.g. 2 variables for MI, 3 for CMI)</div></blockquote>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>n_perm surrogates of the average MI/CMI/TE over all samples
under the null hypothesis of no relationship between var1 and
var2 (in the context of conditional)</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-idtxl.estimators_opencl">
<span id="idtxl-estimators-opencl-module"></span><h2>idtxl.estimators_opencl module<a class="headerlink" href="#module-idtxl.estimators_opencl" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="idtxl.estimators_opencl.OpenCLKraskov">
<em class="property">class </em><code class="descclassname">idtxl.estimators_opencl.</code><code class="descname">OpenCLKraskov</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_opencl.html#OpenCLKraskov"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_opencl.OpenCLKraskov" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract class for implementation of OpenCL estimators.</p>
<p>Abstract class for implementation of OpenCL estimators, child classes
implement estimators for mutual information (MI) and conditional mutual
information (CMI) using the Kraskov-Grassberger-Stoegbauer estimator for
continuous data.</p>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>Kraskov, A., Stoegbauer, H., &amp; Grassberger, P. (2004). Estimating
mutual information. Phys Rev E, 69(6), 066138.</li>
<li>Lizier, Joseph T., Mikhail Prokopenko, and Albert Y. Zomaya. (2012).
Local measures of information storage in complex distributed
computation. Inform Sci, 208, 39-54.</li>
<li>Schreiber, T. (2000). Measuring information transfer. Phys Rev Lett,
85(2), 461.</li>
</ul>
</div></blockquote>
<p>Estimators can be used to perform multiple, independent searches in
parallel. Each of these parallel searches is called a &#8216;chunk&#8217;. To search
multiple chunks, provide point sets as 2D arrays, where the first
dimension represents samples or points, and the second dimension
represents the points&#8217; dimensions. Concatenate chunk data in the first
dimension and pass the number of chunks to the estimators. Chunks must be
of equal size.</p>
<p>Set common estimation parameters for OpenCL estimators. For usage of these
estimators see documentation for the child classes.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;gpuid&#8217; - device ID used for estimation (if more than one device
is available on the current platform) (default=0)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;debug&#8217; - return intermediate results, i.e. neighbour counts from
range searches and KNN distances (default=False)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_opencl.OpenCLKraskovCMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_opencl.</code><code class="descname">OpenCLKraskovCMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_opencl.html#OpenCLKraskovCMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_opencl.OpenCLKraskovCMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate conditional mutual inform with OpenCL Kraskov implementation.</p>
<p>Calculate the conditional mutual information (CMI) between three variables
using OpenCL GPU-code. If no conditional is given (is None), the function
returns the mutual information between var1 and var2. See parent class for
references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;gpuid&#8217; - device ID used for estimation (if more than one device
is available on the current platform) (default=0)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;debug&#8217; - return intermediate results, i.e. neighbour counts from
range searches and KNN distances (default=False)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_opencl.OpenCLKraskovCMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>conditional=None</em>, <em>n_chunks=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_opencl.html#OpenCLKraskovCMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_opencl.OpenCLKraskovCMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate conditional mutual information.</p>
<p>If conditional is None, the mutual information between var1 and var2 is
calculated.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, a 2D numpy array where array
dimensions represent [(realisations * n_chunks) x variable
dimension], array type should be int32</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>conditional <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of conditioning variable (similar to var1)</dd>
<dt>n_chunks <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of data chunks, no. data points has to be the same for
each chunk</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average CMI over all samples or local CMI for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_opencl.OpenCLKraskovMI">
<em class="property">class </em><code class="descclassname">idtxl.estimators_opencl.</code><code class="descname">OpenCLKraskovMI</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_opencl.html#OpenCLKraskovMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_opencl.OpenCLKraskovMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate mutual information with OpenCL Kraskov implementation.</p>
<p>Calculate the mutual information (MI) between two variables using OpenCL
GPU-code. See parent class for references.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict [optional]</span></dt>
<dd><p class="first">set estimator parameters:</p>
<ul class="last simple">
<li>&#8216;gpuid&#8217; - device ID used for estimation (if more than one device
is available on the current platform) (default=0)</li>
<li>&#8216;kraskov_k&#8217; - no. nearest neighbours for KNN search (default=4)</li>
<li>&#8216;normalise&#8217; - z-standardise data (default=False)</li>
<li>&#8216;theiler_t&#8217; - no. next temporal neighbours ignored in KNN and
range searches (default=&#8216;0&#8217;)</li>
<li>&#8216;noise_level&#8217; - random noise added to the data (default=&#8216;1e-8&#8217;)</li>
<li>&#8216;debug&#8217; - return intermediate results, i.e. neighbour counts from
range searches and KNN distances (default=False)</li>
<li>&#8216;lag&#8217; - time difference in samples to calculate the lagged MI
between processes (default=0)</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_opencl.OpenCLKraskovMI.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>var1</em>, <em>var2</em>, <em>n_chunks=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_opencl.html#OpenCLKraskovMI.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_opencl.OpenCLKraskovMI.estimate" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate mutual information.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>var1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of first variable, a 2D numpy array where array
dimensions represent [(realisations * n_chunks) x variable
dimension], array type should be int32</dd>
<dt>var2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>realisations of the second variable (similar to var1)</dd>
<dt>n_chunks <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>number of data chunks, no. data points has to be the same for
each chunk</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>float | numpy array</dt>
<dd>average MI over all samples or local MI for individual
samples if &#8216;local_values&#8217;=True</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-idtxl.estimators_pid">
<span id="idtxl-estimators-pid-module"></span><h2>idtxl.estimators_pid module<a class="headerlink" href="#module-idtxl.estimators_pid" title="Permalink to this headline">¶</a></h2>
<p>Partical information decomposition for discrete random variables.</p>
<p>This module provides an estimator for partial information decomposition
as proposed in</p>
<p>Bertschinger, Rauh, Olbrich, Jost, Ay; Quantifying Unique Information,
Entropy 2014, 16, 2161-2183; doi:10.3390/e16042161</p>
<dl class="class">
<dt id="idtxl.estimators_pid.SydneyPID">
<em class="property">class </em><code class="descclassname">idtxl.estimators_pid.</code><code class="descname">SydneyPID</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_pid.html#SydneyPID"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_pid.SydneyPID" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate partial information decomposition of discrete variables.</p>
<p>Fast implementation of the partial information decomposition (PID)
estimator for discrete data. The estimator does not require JAVA or GPU
modules to run.</p>
<p>The estimator finds shared information, unique information and
synergistic information between the two inputs s1 and s2 with respect to
the output t.</p>
<p>Improved version with larger initial swaps and checking for convergence of
both the unique information from sources 1 and 2. The function counts the
empirical observations, calculates probabilities and the initial CMI, then
does the vitrualised swaps until it has converged, and finally calculates
the PID. The virtualised swaps stage contains two loops. An inner loop
which actually does the virtualised swapping, keeping the changes if the
CMI decreases; and an outer loop which decreases the size of the
probability mass increment the virtualised swapping utilises.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">estimation parameters</p>
<ul class="last simple">
<li>&#8216;alph_s1&#8217; - alphabet size of s1</li>
<li>&#8216;alph_s2&#8217; -  alphabet size of s2</li>
<li>&#8216;alph_t&#8217; - alphabet size of t</li>
<li>&#8216;max_unsuc_swaps_row_parm&#8217; - soft limit for virtualised swaps
based on the number of unsuccessful swaps attempted in a row.
If there are too many unsuccessful swaps in a row, then it
will break the inner swap loop; the outer loop decrements the
size of the probability mass increment and then attemps
virtualised swaps again with the smaller probability increment.
The exact number of unsuccessful swaps allowed before breaking
is the total number of possible swaps (given our alphabet
sizes) times the control parameter &#8216;max_unsuc_swaps_row_parm&#8217;,
e.g., if the parameter is set to 3, this gives a high degree of
confidence that nearly (if not) all of the possible swaps have
been attempted before this soft limit breaks the swap loop.</li>
<li>&#8216;num_reps&#8217; -  number of times the outer loop will halve the
size of the probability increment used for the virtualised
swaps. This is in direct correspondence with the number of times
the empirical data was replicated in your original
implementation.</li>
<li>&#8216;max_iters&#8217; - provides a hard upper bound on the number of times
it will attempt to perform virtualised swaps in the inner loop.
However, this hard limit is (practically) never used as it should
always hit the soft limit defined above (parameter may be removed
in the future).</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_pid.SydneyPID.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>s1</em>, <em>s2</em>, <em>t</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_pid.html#SydneyPID.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_pid.SydneyPID.estimate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>s1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable</dd>
<dt>s2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable</dd>
<dt>t <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>dict</dt>
<dd>estimated decomposition, contains the joint distribution,
unique, shared, and synergistic information</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="idtxl.estimators_pid.TartuPID">
<em class="property">class </em><code class="descclassname">idtxl.estimators_pid.</code><code class="descname">TartuPID</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_pid.html#TartuPID"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_pid.TartuPID" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate partial information decomposition for two inputs and one output</p>
<p>Fast implementation of the partial information decomposition (PID)
estimator for discrete data. The estimator does require a gurobi
installation.</p>
<p>The estimator finds shared information, unique information and
synergistic information between the two inputs s1 and s2 with respect to
the output t.</p>
<p>Improved version with larger initial swaps and checking for convergence of
both the unique information from sources 1 and 2.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">estimation parameters (with default parameters)</p>
<ul class="last simple">
<li>&#8216;get_sorted_pdf&#8217; -False</li>
<li>&#8216;true_pdf&#8217; - None</li>
<li>&#8216;true_result&#8217; - None</li>
<li>&#8216;true_CI&#8217; - None</li>
<li>&#8216;true_SI&#8217; - None</li>
<li>&#8216;feas_eps&#8217; - 1.e-10</li>
<li>&#8216;kkt_eps&#8217; - 1.e-5</li>
<li>&#8216;feas_eps_2&#8217; - 1.e-6</li>
<li>&#8216;kkt_eps_2&#8217; - 0.01</li>
<li>&#8216;kkt_search_eps&#8217; - 0.5</li>
<li>&#8216;max_zero_probability&#8217; - 1.e-5</li>
<li>&#8216;verbose&#8217; - False</li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="idtxl.estimators_pid.TartuPID.estimate">
<code class="descname">estimate</code><span class="sig-paren">(</span><em>s1</em>, <em>s2</em>, <em>t</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_pid.html#TartuPID.estimate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_pid.TartuPID.estimate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>s1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable</dd>
<dt>s2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable</dd>
<dt>t <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>dict</dt>
<dd>estimated decomposition, contains the optimised PDF, shared,
and synergistic information</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="idtxl.estimators_pid.pid_frankfurt">
<code class="descclassname">idtxl.estimators_pid.</code><code class="descname">pid_frankfurt</code><span class="sig-paren">(</span><em>self</em>, <em>s1</em>, <em>s2</em>, <em>t</em>, <em>settings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/idtxl/estimators_pid.html#pid_frankfurt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#idtxl.estimators_pid.pid_frankfurt" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate partial information decomposition of discrete variables.</p>
<p>The pid estimator returns estimates of shared information, unique
information and synergistic information that two random variables X and
Y have about a third variable Z. The estimator finds these estimates by
permuting the initial joint probability distribution of X, Y, and Z to
find a permuted distribution Q that minimizes the unique information in
X about Z (as proposed by Bertschinger and colleagues). The unique in-
formation is defined as the conditional mutual information I(X;Z|Y).</p>
<p>The estimator iteratively permutes the joint probability distribution of
X, Y, and Z under the constraint that the marginal distributions (X, Z)
and (Y, Z) stay constant. This is done by swapping two realizations of X
which have the same corresponding value in Z, e.g.:</p>
<blockquote>
<div><p>X [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]
Y [0, 0, 1, 1, 1, 0, 0, 0, 1, 1]
&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;
Z [1, 1, 0, 0, 0, 1, 1, 0, 1, 0]</p>
<p>Possible swaps: X[0] and X[1]; X[0] and X[4]; X[2] and X[8]; ...</p>
</div></blockquote>
<p>After each swap, I(X;Z|Y) is re-calculated under the new distribution;
if the CMI is lower than the current permutation is kept and the next
swap is tested. The iteration stops after the provided number of
iterations.</p>
<dl class="docutils">
<dt>Example:</dt>
<dd><p class="first">import numpy as np
import pid</p>
<p>n = 5000
alph = 2
x = np.random.randint(0, alph, n)
y = np.random.randint(0, alph, n)
z = np.logical_xor(x, y).astype(int)
cfg = {</p>
<blockquote>
<div>&#8216;alphabetsize&#8217;: 2,
&#8216;jarpath&#8217;: &#8216;/home/user/infodynamics-dist-1.3/infodynamics.jar&#8217;,
&#8216;iterations&#8217;: 10000</div></blockquote>
<p class="last">}
[est, opt] = pid(x, y, z, cfg)</p>
</dd>
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>s1 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable
(this is the source variable the algorithm calculates the actual
UI for)</dd>
<dt>s2 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable (the
other source variable)</dd>
<dt>t <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>1D array containing realizations of a discrete random variable</dd>
<dt>settings <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd><p class="first">estimation parameters</p>
<ul class="last simple">
<li>&#8216;alphabetsize&#8217; - no. values in each variable s1, s2, t</li>
<li>&#8216;jarpath&#8217; - string with path to JIDT jar file</li>
<li>&#8216;iterations&#8217; - no. iterations of the estimator</li>
</ul>
</dd>
</dl>
</dd>
<dt>Returns:</dt>
<dd><dl class="first last docutils">
<dt>dict</dt>
<dd>estimated decomposition, contains: MI/CMI values computed
from non-permuted distributions; PID estimates (shared,
synergistic, unique information); I(target;s1,s2) under permuted
distribution Q</dd>
<dt>dict</dt>
<dd>additional information about iterative optimization,
contains: final permutation Q; settings dictionary; array with
I(target:s1|s2) for each iteration; array with delta
I(target:s1|s2) for each iteration; I(target:s1,s2) for each
iteration</dd>
</dl>
</dd>
<dt>Note:</dt>
<dd>variables names joined by &#8220;_&#8221; enter a mutual information computation
together i.e. mi_va1_var2 &#8211;&gt; I(var1 : var2). Variables names joined
directly form a new joint variable
mi_var1var2_var3 &#8211;&gt; I(var3:(var1,var2))</dd>
</dl>
</dd></dl>

</div>
</div>


                </div>
              </div>
            </div>
          </div>
        </div>
         
<div class="col-md-3 hidden-xs" id="sidebar-wrapper">
  <div class="sidebar hidden-xs" role="navigation" aria-label="main navigation">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Information theoretic estimators</a><ul>
<li><a class="reference internal" href="#module-idtxl.estimators_jidt">idtxl.estimators_jidt module</a></li>
<li><a class="reference internal" href="#module-idtxl.estimators_opencl">idtxl.estimators_opencl module</a></li>
<li><a class="reference internal" href="#module-idtxl.estimators_pid">idtxl.estimators_pid module</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="idtxl_network_comparison.html"
                        title="previous chapter">Algorithms for network comparison</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="idtxl_helper.html"
                        title="next chapter">Helper functions</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/idtxl_estimators.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" role="search">
  <h3>Quick search</h3>
  <form class="search form-inline" action="search.html" method="get">
      <div class="input-append input-group">
        <input type="text" class="search-query form-control" name="q" placeholder="Search...">
        <span class="input-group-btn">
        <input type="submit" class="btn" value="Go" />
        </span>
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row footer-relbar">
<div id="navbar-related" class=" related navbar navbar-default" role="navigation" aria-label="related navigation">
  <div class="navbar-inner">
    <ul class="nav navbar-nav ">
        <li><a href="index.html">IDTxl 0.1 documentation</a></li>
    </ul>
<ul class="nav navbar-nav pull-right hidden-xs hidden-sm">
      
        <li><a href="idtxl_network_comparison.html" title="Algorithms for network comparison" >previous</a></li>
        <li><a href="idtxl_helper.html" title="Helper functions" >next</a></li>
        <li><a href="py-modindex.html" title="Python Module Index" >modules</a></li>
        <li><a href="genindex.html" title="General Index" >index</a></li>
        <li><a href="#">top</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer role="contentinfo">
          &copy; Copyright 2016, Patricia Wollstadt, Michael Wibral, Joe T. Lizier, Finn Connor, Raul Vicente.
        Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.3.6.
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>